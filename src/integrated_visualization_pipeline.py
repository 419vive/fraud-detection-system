"""
é›†æˆå¯è¦–åŒ–æµæ°´ç·š - IEEE-CIS è©é¨™æª¢æ¸¬é …ç›®
æ•´åˆç‰¹å¾µå·¥ç¨‹ã€æ¨¡å‹è¨“ç·´ã€å¯è¦–åŒ–åˆ†æçš„å®Œæ•´æµæ°´ç·š
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime
import os
import json
import pickle
from pathlib import Path

# å°å…¥ç¾æœ‰æ¨¡çµ„
from .feature_engineering import FeatureEngineer, engineer_features
from .data_validation import DataValidator
from .model_monitoring import ModelMonitor
from .visualization_engine import VisualizationEngine
from .realtime_dashboard import RealTimeMonitoringSystem
from .model_comparison_viz import ModelComparisonVisualizer
from .business_analytics import BusinessAnalyzer
from .config import get_config

# æ©Ÿå™¨å­¸ç¿’æ¨¡çµ„
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import xgboost as xgb
import lightgbm as lgb

logger = logging.getLogger(__name__)

class IntegratedVisualizationPipeline:
    """é›†æˆå¯è¦–åŒ–æµæ°´ç·š"""
    
    def __init__(self, config_manager=None, output_base_dir: str = 'fraud_detection_output'):
        self.config = config_manager or get_config()
        self.output_base_dir = Path(output_base_dir)
        self.output_base_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å„å€‹çµ„ä»¶
        self.data_validator = DataValidator(config_manager)
        self.feature_engineer = FeatureEngineer(config_manager)
        self.viz_engine = VisualizationEngine(config_manager)
        self.model_comparator = ModelComparisonVisualizer(config_manager)
        self.business_analyzer = BusinessAnalyzer(config_manager)
        
        # å­˜å„²çµæœ
        self.validation_results = {}
        self.feature_engineering_results = {}
        self.model_results = {}
        self.visualization_outputs = {}
        
        # é‹è¡Œçµ±è¨ˆ
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_time': None,
            'stages_completed': [],
            'errors': []
        }
    
    def run_complete_pipeline(self, 
                            df: pd.DataFrame,
                            target_col: str = 'isFraud',
                            test_size: float = 0.2,
                            enable_realtime_monitoring: bool = False,
                            models_to_train: List[str] = None) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„å¯è¦–åŒ–åˆ†ææµæ°´ç·š"""
        
        self.pipeline_stats['start_time'] = datetime.now()
        logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œé›†æˆå¯è¦–åŒ–æµæ°´ç·š - {self.pipeline_stats['start_time']}")
        
        try:
            # éšæ®µ1: æ•¸æ“šé©—è­‰
            logger.info("ğŸ“Š éšæ®µ1: æ•¸æ“šé©—è­‰èˆ‡å“è³ªåˆ†æ")
            validation_output = self._run_data_validation(df, target_col)
            self.pipeline_stats['stages_completed'].append('data_validation')
            
            # éšæ®µ2: ç‰¹å¾µå·¥ç¨‹
            logger.info("ğŸ”§ éšæ®µ2: ç‰¹å¾µå·¥ç¨‹èˆ‡æ•¸æ“šé è™•ç†")
            processed_df = self._run_feature_engineering(df, target_col)
            self.pipeline_stats['stages_completed'].append('feature_engineering')
            
            # éšæ®µ3: æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ
            logger.info("ğŸ¤– éšæ®µ3: æ¨¡å‹è¨“ç·´èˆ‡æ€§èƒ½æ¯”è¼ƒ")
            model_results = self._run_model_training_and_comparison(
                processed_df, target_col, test_size, models_to_train
            )
            self.pipeline_stats['stages_completed'].append('model_training')
            
            # éšæ®µ4: å¯è¦–åŒ–åˆ†æ
            logger.info("ğŸ“ˆ éšæ®µ4: ç¶œåˆå¯è¦–åŒ–åˆ†æ")
            visualization_outputs = self._run_visualization_analysis(
                df, processed_df, model_results, target_col
            )
            self.pipeline_stats['stages_completed'].append('visualization')
            
            # éšæ®µ5: å•†æ¥­åˆ†æ
            logger.info("ğŸ’¼ éšæ®µ5: å•†æ¥­æ´å¯Ÿèˆ‡è²¡å‹™åˆ†æ")
            business_outputs = self._run_business_analysis(
                processed_df, model_results, target_col
            )
            self.pipeline_stats['stages_completed'].append('business_analysis')
            
            # éšæ®µ6: å¯¦æ™‚ç›£æ§ï¼ˆå¯é¸ï¼‰
            if enable_realtime_monitoring:
                logger.info("âš¡ éšæ®µ6: å¯¦æ™‚ç›£æ§ç³»çµ±è¨­ç½®")
                monitoring_setup = self._setup_realtime_monitoring(model_results)
                self.pipeline_stats['stages_completed'].append('realtime_monitoring')
            
            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            logger.info("ğŸ“‹ ç”Ÿæˆæœ€çµ‚ç¶œåˆå ±å‘Š")
            final_report = self._generate_final_report()
            
            self.pipeline_stats['end_time'] = datetime.now()
            self.pipeline_stats['total_time'] = (
                self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']
            ).total_seconds()
            
            logger.info(f"âœ… æµæ°´ç·šåŸ·è¡Œå®Œæˆ - ç¸½è€—æ™‚: {self.pipeline_stats['total_time']:.2f}ç§’")
            
            return {
                'pipeline_stats': self.pipeline_stats,
                'validation_results': validation_output,
                'model_results': model_results,
                'visualization_outputs': visualization_outputs,
                'business_outputs': business_outputs,
                'final_report': final_report
            }
            
        except Exception as e:
            self.pipeline_stats['errors'].append(str(e))
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—: {e}")
            raise
    
    def _run_data_validation(self, df: pd.DataFrame, target_col: str) -> Dict[str, str]:
        """é‹è¡Œæ•¸æ“šé©—è­‰"""
        validation_dir = self.output_base_dir / 'data_validation'
        validation_dir.mkdir(exist_ok=True)
        
        # åŸ·è¡Œå…¨é¢æ•¸æ“šé©—è­‰
        validation_files = self.data_validator.comprehensive_data_validation_report(
            df, target_col, str(validation_dir)
        )
        
        self.validation_results = validation_files
        logger.info(f"æ•¸æ“šé©—è­‰å®Œæˆï¼Œç”Ÿæˆ {len(validation_files)} å€‹å ±å‘Šæ–‡ä»¶")
        
        return validation_files
    
    def _run_feature_engineering(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """é‹è¡Œç‰¹å¾µå·¥ç¨‹"""
        logger.info("åŸ·è¡Œå„ªåŒ–çš„ç‰¹å¾µå·¥ç¨‹æµæ°´ç·š...")
        
        # ä½¿ç”¨ç¾æœ‰çš„ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„
        processed_df, summary = engineer_features(
            df, target_col, enable_parallel=True, enable_advanced=True
        )
        
        # ä¿å­˜ç‰¹å¾µå·¥ç¨‹çµæœ
        fe_dir = self.output_base_dir / 'feature_engineering'
        fe_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
        processed_df.to_csv(fe_dir / 'processed_data.csv', index=False)
        
        # ä¿å­˜ç‰¹å¾µå·¥ç¨‹æ‘˜è¦
        with open(fe_dir / 'feature_engineering_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        self.feature_engineering_results = {
            'summary': summary,
            'processed_data_path': str(fe_dir / 'processed_data.csv'),
            'original_features': len(df.columns),
            'engineered_features': len(processed_df.columns),
            'features_added': len(processed_df.columns) - len(df.columns)
        }
        
        logger.info(f"ç‰¹å¾µå·¥ç¨‹å®Œæˆ - åŸå§‹ç‰¹å¾µ: {len(df.columns)}, å·¥ç¨‹å¾Œç‰¹å¾µ: {len(processed_df.columns)}")
        
        return processed_df
    
    def _run_model_training_and_comparison(self, 
                                         df: pd.DataFrame, 
                                         target_col: str,
                                         test_size: float,
                                         models_to_train: List[str] = None) -> Dict[str, Any]:
        """é‹è¡Œæ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ"""
        
        if models_to_train is None:
            models_to_train = ['random_forest', 'xgboost', 'lightgbm', 'logistic']
        
        # æº–å‚™æ•¸æ“š
        feature_cols = [col for col in df.columns if col != target_col]
        X = df[feature_cols].select_dtypes(include=[np.number])  # åªä½¿ç”¨æ•¸å€¼ç‰¹å¾µ
        y = df[target_col]
        
        # åˆ†å‰²æ•¸æ“š
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        logger.info(f"æ•¸æ“šåˆ†å‰²å®Œæˆ - è¨“ç·´é›†: {len(X_train)}, æ¸¬è©¦é›†: {len(X_test)}")
        
        # å®šç¾©æ¨¡å‹
        models = {}
        if 'random_forest' in models_to_train:
            models['Random Forest'] = RandomForestClassifier(
                **self.config.get_model_params('random_forest')
            )
        
        if 'logistic' in models_to_train:
            models['Logistic Regression'] = LogisticRegression(
                **self.config.get_model_params('logistic')
            )
        
        if 'xgboost' in models_to_train:
            models['XGBoost'] = xgb.XGBClassifier(
                **self.config.get_model_params('xgboost'),
                verbosity=0
            )
        
        if 'lightgbm' in models_to_train:
            models['LightGBM'] = lgb.LGBMClassifier(
                **self.config.get_model_params('lightgbm')
            )
        
        # è¨“ç·´å’Œè©•ä¼°æ¨¡å‹
        model_results = {}
        model_dir = self.output_base_dir / 'models'
        model_dir.mkdir(exist_ok=True)
        
        for name, model in models.items():
            logger.info(f"è¨“ç·´ {name}...")
            
            # è¨“ç·´æ¨¡å‹
            start_time = datetime.now()
            model.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            # é æ¸¬
            start_time = datetime.now()
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            prediction_time = (datetime.now() - start_time).total_seconds()
            
            # äº¤å‰é©—è­‰
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=3, scoring='roc_auc', n_jobs=-1
            )
            
            # ç‰¹å¾µé‡è¦æ€§
            feature_importance = None
            if hasattr(model, 'feature_importances_'):
                feature_importance = model.feature_importances_
            elif hasattr(model, 'coef_'):
                feature_importance = np.abs(model.coef_[0])
            
            # ä¿å­˜æ¨¡å‹
            model_path = model_dir / f'{name.lower().replace(" ", "_")}_model.pkl'
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            
            # è¨ˆç®—æŒ‡æ¨™
            auc_score = roc_auc_score(y_test, y_pred_proba)
            
            # å­˜å„²çµæœ
            model_results[name] = {
                'model': model,
                'model_path': str(model_path),
                'y_true': y_test.values,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba,
                'feature_importance': feature_importance,
                'feature_names': X.columns.tolist(),
                'training_time': training_time,
                'prediction_time': prediction_time,
                'auc_score': auc_score,
                'cv_scores': cv_scores.tolist(),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            logger.info(f"{name} - AUC: {auc_score:.4f}, CV AUC: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
        
        # æ¨¡å‹æ¯”è¼ƒåˆ†æ
        comparison_dir = self.output_base_dir / 'model_comparison'
        comparison_dir.mkdir(exist_ok=True)
        
        # æ·»åŠ æ¨¡å‹çµæœåˆ°æ¯”è¼ƒå™¨
        for name, results in model_results.items():
            self.model_comparator.add_model_results(name, results)
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        comparison_dashboard = self.model_comparator.create_comprehensive_comparison_dashboard()
        comparison_dashboard.write_html(comparison_dir / 'model_comparison_dashboard.html')
        
        feature_analysis = self.model_comparator.create_feature_importance_analysis()
        feature_analysis.write_html(comparison_dir / 'feature_importance_analysis.html')
        
        comparison_report = self.model_comparator.generate_comparison_report(
            str(comparison_dir / 'comparison_report.json')
        )
        
        self.model_results = model_results
        
        return {
            'models': model_results,
            'comparison_report': comparison_report,
            'best_model': max(model_results.items(), key=lambda x: x[1]['auc_score']),
            'comparison_files': {
                'dashboard': str(comparison_dir / 'model_comparison_dashboard.html'),
                'feature_analysis': str(comparison_dir / 'feature_importance_analysis.html'),
                'report': str(comparison_dir / 'comparison_report.json')
            }
        }
    
    def _run_visualization_analysis(self, 
                                  original_df: pd.DataFrame,
                                  processed_df: pd.DataFrame,
                                  model_results: Dict[str, Any],
                                  target_col: str) -> Dict[str, str]:
        """é‹è¡Œå¯è¦–åŒ–åˆ†æ"""
        
        viz_dir = self.output_base_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        # ç²å–æœ€ä½³æ¨¡å‹çµæœ
        best_model_name, best_model_data = model_results['best_model']
        
        # 1. æ¨¡å‹æ€§èƒ½å„€è¡¨æ¿
        performance_fig = self.viz_engine.create_model_performance_dashboard(
            best_model_data['y_true'],
            best_model_data['y_pred'],
            best_model_data['y_pred_proba']
        )
        performance_path = viz_dir / 'model_performance_dashboard.html'
        performance_fig.write_html(performance_path)
        
        # 2. äº¤æ˜“æ¨¡å¼åˆ†æ
        pattern_fig = self.viz_engine.create_transaction_pattern_analysis(original_df)
        pattern_path = viz_dir / 'transaction_patterns_analysis.html'
        pattern_fig.write_html(pattern_path)
        
        # 3. åœ°ç†åˆ†æ
        geo_fig = self.viz_engine.create_geographic_analysis(original_df)
        geo_path = viz_dir / 'geographic_analysis.html'
        geo_fig.write_html(geo_path)
        
        # 4. ç¶œåˆå ±å‘Š
        comprehensive_reports = self.viz_engine.create_comprehensive_report(
            original_df, best_model_data, str(viz_dir / 'comprehensive_analysis')
        )
        
        visualization_outputs = {
            'model_performance': str(performance_path),
            'transaction_patterns': str(pattern_path),
            'geographic_analysis': str(geo_path),
            'comprehensive_reports': comprehensive_reports
        }
        
        self.visualization_outputs = visualization_outputs
        
        return visualization_outputs
    
    def _run_business_analysis(self, 
                             df: pd.DataFrame,
                             model_results: Dict[str, Any],
                             target_col: str) -> Dict[str, str]:
        """é‹è¡Œå•†æ¥­åˆ†æ"""
        
        business_dir = self.output_base_dir / 'business_analysis'
        business_dir.mkdir(exist_ok=True)
        
        # ç²å–æœ€ä½³æ¨¡å‹çš„é æ¸¬çµæœ
        best_model_name, best_model_data = model_results['best_model']
        
        # å°æ•´å€‹æ•¸æ“šé›†é€²è¡Œé æ¸¬ï¼ˆç”¨æ–¼å•†æ¥­åˆ†æï¼‰
        model = best_model_data['model']
        feature_cols = [col for col in df.columns if col != target_col]
        X_full = df[feature_cols].select_dtypes(include=[np.number])
        
        full_predictions = model.predict(X_full)
        full_prediction_probabilities = model.predict_proba(X_full)[:, 1]
        
        # å‰µå»ºå•†æ¥­åˆ†æå„€è¡¨æ¿
        business_dashboards = self.business_analyzer.create_comprehensive_business_dashboard(
            df, full_predictions, full_prediction_probabilities, str(business_dir)
        )
        
        return business_dashboards
    
    def _setup_realtime_monitoring(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¨­ç½®å¯¦æ™‚ç›£æ§ç³»çµ±"""
        
        monitoring_dir = self.output_base_dir / 'realtime_monitoring'
        monitoring_dir.mkdir(exist_ok=True)
        
        # ç²å–æœ€ä½³æ¨¡å‹
        best_model_name, best_model_data = model_results['best_model']
        
        # å‰µå»ºç›£æ§ç³»çµ±é…ç½®
        monitoring_config = {
            'model_name': best_model_name,
            'model_path': best_model_data['model_path'],
            'feature_names': best_model_data['feature_names'],
            'monitoring_port': 8051,
            'alert_thresholds': {
                'fraud_rate': 0.05,
                'model_performance': 0.85,
                'system_latency': 1.0,
                'data_quality': 0.95
            }
        }
        
        # ä¿å­˜é…ç½®
        config_path = monitoring_dir / 'monitoring_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(monitoring_config, f, ensure_ascii=False, indent=2)
        
        # å‰µå»ºç›£æ§å•Ÿå‹•è…³æœ¬
        startup_script = f"""
#!/usr/bin/env python
# å¯¦æ™‚ç›£æ§ç³»çµ±å•Ÿå‹•è…³æœ¬

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.realtime_dashboard import RealTimeMonitoringSystem
import json

# è¼‰å…¥é…ç½®
with open('{config_path}', 'r', encoding='utf-8') as f:
    config = json.load(f)

# å‰µå»ºç›£æ§ç³»çµ±
monitor = RealTimeMonitoringSystem()

# å•Ÿå‹•ç›£æ§
monitor.start_monitoring(dashboard_port=config['monitoring_port'])

print(f"å¯¦æ™‚ç›£æ§ç³»çµ±å·²å•Ÿå‹• - è¨ªå• http://localhost:{{config['monitoring_port']}}")
print("æŒ‰ Ctrl+C åœæ­¢ç³»çµ±")

try:
    monitor.run_dashboard(port=config['monitoring_port'])
except KeyboardInterrupt:
    print("åœæ­¢ç›£æ§ç³»çµ±...")
    monitor.stop_monitoring()
"""
        
        script_path = monitoring_dir / 'start_monitoring.py'
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(startup_script)
        
        # ä½¿è…³æœ¬å¯åŸ·è¡Œ
        os.chmod(script_path, 0o755)
        
        return {
            'config_path': str(config_path),
            'startup_script': str(script_path),
            'monitoring_port': monitoring_config['monitoring_port']
        }
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚ç¶œåˆå ±å‘Š"""
        
        report = {
            'generation_time': datetime.now().isoformat(),
            'pipeline_execution': self.pipeline_stats,
            'data_summary': {
                'validation_files': len(self.validation_results),
                'feature_engineering': self.feature_engineering_results,
                'models_trained': len(self.model_results.get('models', {})),
                'visualization_outputs': len(self.visualization_outputs)
            },
            'best_model_performance': None,
            'key_insights': [],
            'recommendations': [],
            'output_files': {}
        }
        
        # æœ€ä½³æ¨¡å‹æ€§èƒ½
        if 'models' in self.model_results:
            best_model_name, best_model_data = self.model_results['best_model']
            report['best_model_performance'] = {
                'model_name': best_model_name,
                'auc_score': best_model_data['auc_score'],
                'cv_mean': best_model_data['cv_mean'],
                'cv_std': best_model_data['cv_std']
            }
        
        # é—œéµæ´å¯Ÿ
        if report['best_model_performance']:
            auc = report['best_model_performance']['auc_score']
            if auc > 0.9:
                report['key_insights'].append("ğŸ¯ æ¨¡å‹æ€§èƒ½å„ªç•°ï¼ŒAUCè¶…é0.9")
            elif auc > 0.8:
                report['key_insights'].append("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼ŒAUCè¶…é0.8")
            else:
                report['key_insights'].append("âš ï¸ æ¨¡å‹æ€§èƒ½éœ€è¦æå‡")
        
        if 'features_added' in self.feature_engineering_results:
            features_added = self.feature_engineering_results['features_added']
            report['key_insights'].append(f"ğŸ”§ ç‰¹å¾µå·¥ç¨‹æ–°å¢äº† {features_added} å€‹ç‰¹å¾µ")
        
        # å»ºè­°
        report['recommendations'].extend([
            "å®šæœŸé‡æ–°è¨“ç·´æ¨¡å‹ä»¥ä¿æŒæ€§èƒ½",
            "ç›£æ§æ¨¡å‹åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­çš„è¡¨ç¾",
            "æŒçºŒæ”¶é›†æ–°çš„ç‰¹å¾µä»¥æ”¹å–„æª¢æ¸¬èƒ½åŠ›",
            "å»ºç«‹å®Œå–„çš„è­¦å ±æ©Ÿåˆ¶"
        ])
        
        # è¼¸å‡ºæ–‡ä»¶æ•´ç†
        all_outputs = {}
        all_outputs.update(self.validation_results)
        all_outputs.update(self.visualization_outputs)
        if 'comparison_files' in self.model_results:
            all_outputs.update(self.model_results['comparison_files'])
        
        report['output_files'] = all_outputs
        
        # ä¿å­˜æœ€çµ‚å ±å‘Š
        report_path = self.output_base_dir / 'final_comprehensive_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # å‰µå»ºHTMLç‰ˆæœ¬çš„å ±å‘Š
        self._create_html_final_report(report)
        
        return report
    
    def _create_html_final_report(self, report: Dict[str, Any]):
        """å‰µå»ºHTMLç‰ˆæœ¬çš„æœ€çµ‚å ±å‘Š"""
        
        best_model = report.get('best_model_performance', {})
        pipeline_stats = report.get('pipeline_execution', {})
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>è©é¨™æª¢æ¸¬ç³»çµ± - ç¶œåˆåˆ†æå ±å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }}
                .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
                .header {{ background: white; padding: 40px; border-radius: 20px; box-shadow: 0 10px 40px rgba(0,0,0,0.1); margin-bottom: 30px; text-align: center; }}
                .header h1 {{ color: #2c3e50; margin: 0; font-size: 2.5em; }}
                .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }}
                .stat-card {{ background: white; padding: 25px; border-radius: 15px; box-shadow: 0 5px 20px rgba(0,0,0,0.1); text-align: center; }}
                .stat-value {{ font-size: 2.5em; font-weight: bold; color: #3498db; margin: 10px 0; }}
                .stat-label {{ color: #666; font-size: 0.9em; }}
                .section {{ background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 20px rgba(0,0,0,0.1); margin: 20px 0; }}
                .section h2 {{ color: #2c3e50; margin-top: 0; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                .insights {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 15px; }}
                .insight-item {{ background: #e8f6ff; padding: 15px; border-radius: 10px; border-left: 4px solid #3498db; }}
                .file-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 15px; }}
                .file-item {{ background: #f8f9fa; padding: 15px; border-radius: 10px; }}
                .file-item a {{ color: #3498db; text-decoration: none; font-weight: bold; }}
                .file-item a:hover {{ text-decoration: underline; }}
                .timeline {{ background: #f8f9fa; padding: 20px; border-radius: 10px; }}
                .timeline-item {{ margin: 10px 0; padding: 10px; background: white; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸš€ è©é¨™æª¢æ¸¬ç³»çµ±ç¶œåˆåˆ†æå ±å‘Š</h1>
                    <p>ç”Ÿæˆæ™‚é–“: {report['generation_time']}</p>
                </div>
                
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-value">{best_model.get('auc_score', 0):.3f}</div>
                        <div class="stat-label">æœ€ä½³æ¨¡å‹ AUC åˆ†æ•¸</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{len(report.get('output_files', {}))}</div>
                        <div class="stat-label">ç”Ÿæˆçš„åˆ†ææ–‡ä»¶</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{pipeline_stats.get('total_time', 0):.1f}s</div>
                        <div class="stat-label">ç¸½åŸ·è¡Œæ™‚é–“</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">{len(pipeline_stats.get('stages_completed', []))}</div>
                        <div class="stat-label">å®Œæˆçš„åˆ†æéšæ®µ</div>
                    </div>
                </div>
                
                <div class="section">
                    <h2>ğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½</h2>
                    <p><strong>æ¨¡å‹åç¨±:</strong> {best_model.get('model_name', 'N/A')}</p>
                    <p><strong>AUC åˆ†æ•¸:</strong> {best_model.get('auc_score', 0):.4f}</p>
                    <p><strong>äº¤å‰é©—è­‰å¹³å‡:</strong> {best_model.get('cv_mean', 0):.4f} Â± {best_model.get('cv_std', 0):.4f}</p>
                </div>
                
                <div class="section">
                    <h2>ğŸ” é—œéµæ´å¯Ÿ</h2>
                    <div class="insights">
                        {''.join([f'<div class="insight-item">{insight}</div>' for insight in report.get('key_insights', [])])}
                    </div>
                </div>
                
                <div class="section">
                    <h2>ğŸ’¡ å»ºè­°äº‹é …</h2>
                    <ul>
                        {''.join([f'<li>{rec}</li>' for rec in report.get('recommendations', [])])}
                    </ul>
                </div>
                
                <div class="section">
                    <h2>ğŸ“Š åˆ†ææ–‡ä»¶</h2>
                    <div class="file-grid">
                        <div class="file-item">
                            <h4>ğŸ“ˆ æ•¸æ“šé©—è­‰</h4>
                            <a href="data_validation/data_quality_dashboard.html">æ•¸æ“šå“è³ªå„€è¡¨æ¿</a><br>
                            <a href="data_validation/data_validation_report.md">é©—è­‰å ±å‘Š</a>
                        </div>
                        <div class="file-item">
                            <h4>ğŸ¤– æ¨¡å‹æ¯”è¼ƒ</h4>
                            <a href="model_comparison/model_comparison_dashboard.html">æ¨¡å‹æ¯”è¼ƒå„€è¡¨æ¿</a><br>
                            <a href="model_comparison/feature_importance_analysis.html">ç‰¹å¾µé‡è¦æ€§åˆ†æ</a>
                        </div>
                        <div class="file-item">
                            <h4>ğŸ“Š å¯è¦–åŒ–åˆ†æ</h4>
                            <a href="visualizations/model_performance_dashboard.html">æ€§èƒ½åˆ†æ</a><br>
                            <a href="visualizations/transaction_patterns_analysis.html">äº¤æ˜“æ¨¡å¼</a>
                        </div>
                        <div class="file-item">
                            <h4>ğŸ’¼ å•†æ¥­åˆ†æ</h4>
                            <a href="business_analysis/business_index.html">å•†æ¥­åˆ†æä¸­å¿ƒ</a><br>
                            <a href="business_analysis/financial_impact_dashboard.html">è²¡å‹™å½±éŸ¿</a>
                        </div>
                    </div>
                </div>
                
                <div class="section">
                    <h2>â±ï¸ åŸ·è¡Œæ™‚é–“ç·š</h2>
                    <div class="timeline">
                        {''.join([f'<div class="timeline-item">âœ… {stage.replace("_", " ").title()}</div>' for stage in pipeline_stats.get('stages_completed', [])])}
                    </div>
                </div>
            </div>
        </body>
        </html>
        """
        
        html_path = self.output_base_dir / 'final_comprehensive_report.html'
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"æœ€çµ‚å ±å‘Šå·²ç”Ÿæˆ: {html_path}")

# ä¾¿æ·å‡½æ•¸
def run_integrated_fraud_detection_analysis(df: pd.DataFrame,
                                          target_col: str = 'isFraud',
                                          output_dir: str = 'fraud_detection_analysis',
                                          models_to_train: List[str] = None,
                                          enable_realtime: bool = False) -> Dict[str, Any]:
    """é‹è¡Œå®Œæ•´çš„è©é¨™æª¢æ¸¬åˆ†ææµæ°´ç·š"""
    
    pipeline = IntegratedVisualizationPipeline(output_base_dir=output_dir)
    
    return pipeline.run_complete_pipeline(
        df=df,
        target_col=target_col,
        models_to_train=models_to_train,
        enable_realtime_monitoring=enable_realtime
    )

if __name__ == "__main__":
    print("é›†æˆå¯è¦–åŒ–æµæ°´ç·šå·²è¼‰å…¥å®Œæˆï¼")